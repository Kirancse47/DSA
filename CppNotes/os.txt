üß† 1. Difference between Process and Thread

Answer:
A thread is a single sequence stream within a process. Threads are also called lightweight processes as they possess some of the properties of processes. Each thread belongs to exactly one process.

In an operating system that supports multithreading, a process can consist of many threads. 
All threads belonging to the same process share code section, data section, and OS resources (e.g. open files and signals), but each thread has its own (thread control block) - thread ID, program counter, register set, and a stack.

  
A process is an independent unit of execution with its own address space, code, data, and system resources.

A thread is a lightweight unit of execution within a process ‚Äî threads share the same address space and resources.

Feature	Process	Thread
Memory	Separate	Shared within process
Context Switch	Expensive (MMU switch)	Cheaper (same address space)
Communication	IPC (pipes, sockets, shared memory)	Shared memory (direct access)
Crash impact	Isolated	Can crash entire process

üëâ In C++ terms: If you spawn std::thread inside a process, they share static/global variables. But if you fork(), you get a new process with a copy of address space.

‚öôÔ∏è 2. Difference between Paging and Segmentation

Answer:

Paging: Divides memory into fixed-size blocks (pages & frames).

Segmentation: Divides memory into variable-size logical units (code, data, stack).

Feature	Paging	Segmentation
Size	Fixed	Variable
Basis	Physical memory management	Logical program structure
Fragmentation	Internal	External
Example	Page size = 4KB	Segment size = Code=4KB, Stack=1KB

üëâ Modern OS (Linux) uses both: segmentation for logical separation and paging for physical allocation.

üíæ 3. What is Virtual Memory and why is it used?

Answer:
Virtual memory allows each process to believe it has a large, contiguous memory space, even if physical memory is smaller.
It uses page tables to map virtual pages to physical frames.

Benefits:

Process isolation & security.

Efficient use of RAM via demand paging.

Allows more processes via swapping.

Key OS terms:

Page table ‚Üí mapping of virtual to physical.

TLB ‚Üí cache for page table lookups.

Page fault ‚Üí occurs when page not in RAM ‚Üí OS loads it from disk.

üßÆ 4. What is a Page Fault?

Answer:
A page fault occurs when a process accesses a page that is not present in physical memory.

Steps:

CPU triggers page-fault interrupt.

OS pauses the process.

OS finds the page on disk (swap area).

If RAM is full, it chooses a victim page (via LRU, FIFO, etc.).

Loads required page ‚Üí updates page table ‚Üí resumes process.

Types:

Minor fault: Page is in memory but not mapped.

Major fault: Page is not in memory (needs disk I/O).

üßµ 5. What is a Deadlock? How to prevent it?

Answer:
Deadlock = set of processes waiting indefinitely for resources held by each other.

Necessary conditions (Coffman‚Äôs conditions):

Mutual exclusion

Hold and wait

No preemption

Circular wait

Prevention:

Break any one condition:

Use ordered resource allocation (avoid circular wait).

Preempt resources.

Require all resources upfront (avoid hold-and-wait).

Use timeouts or deadlock detection + recovery.

üîí 6. Semaphore vs Mutex
Feature	Semaphore	Mutex
Value	Integer (count)	Binary (locked/unlocked)
Usage	Resource counting	Mutual exclusion
Ownership	Any thread	Only the thread that locked it
Example	sem_wait(), sem_post()	pthread_mutex_lock(), pthread_mutex_unlock()

In C++ (POSIX):

std::mutex mtx;
void func() {
    std::lock_guard<std::mutex> lock(mtx);
    // critical section
}

üß∞ 7. What is Context Switching?

Answer:
It‚Äôs the process of saving the state (registers, PC, stack pointer, etc.) of a running process/thread and loading the state of another.

Overhead comes from:

Saving/restoring CPU registers.

Updating memory maps (MMU).

Cache invalidation.

Optimization: Use threads over processes to reduce context switch cost since threads share memory space.

üß† 8. What is a Race Condition? How to prevent it?

Answer:
Occurs when multiple threads access shared data concurrently and final outcome depends on the execution order.

Example:

int counter = 0;
void increment() { counter++; }  // not atomic


Prevention:

Use synchronization primitives: mutex, atomic<int>, semaphores.

Example:

std::atomic<int> counter = 0;
void increment() { counter.fetch_add(1); }

üí° 9. Demand Paging

Answer:
In demand paging, OS loads pages only when they‚Äôre needed ‚Äî not in advance.
This reduces memory usage but can cause page faults initially.

Advantages:

Less I/O.

Faster load times.

Efficient memory use.

Disadvantage: Page faults cause latency.

üßÆ 10. Thrashing

Answer:
Thrashing occurs when the CPU spends more time swapping pages in/out of memory than executing instructions.

Cause: Too many active processes ‚Üí not enough RAM ‚Üí constant page faults.
Solution:

Increase RAM.

Reduce degree of multiprogramming.

Use working set model to limit process memory.

‚ö° 11. Preemptive vs Non-preemptive Scheduling
Feature	Preemptive	Non-preemptive
Interrupts	Can interrupt	Cannot interrupt
Response time	Better	Poor
Example	Round Robin	FCFS, SJF (non-preemptive)

Preemptive is used in modern OS (like Linux) for better interactivity.

üìÅ 12. Explain System Calls

Answer:
System calls provide an interface between user space and kernel.

Examples:

read(), write(), fork(), exec(), wait()

When you call printf() ‚Üí

printf() calls write() system call.

Switches to kernel mode.

Kernel writes to device buffer.

Control returns to user mode.

üîã 13. How does the OS handle I/O requests?

Answer:

Requests enter device queue.

OS schedules via I/O schedulers (FCFS, SSTF, SCAN).

Interrupt is raised after completion ‚Üí OS notifies process.

Modern approach: Asynchronous I/O (e.g., io_uring in Linux) to avoid blocking threads.

üß† 14. Difference between User mode and Kernel mode
Feature	User mode	Kernel mode
Access	Limited (no hardware access)	Full system access
System calls	Needed for privileged ops	Direct access
Example	Application code	Device driver, scheduler

Transition: Occurs via traps, interrupts, or syscalls.

üß© 15. What is a Zombie Process?

Answer:
When a child process terminates, but the parent hasn‚Äôt yet called wait(), it becomes a zombie (defunct).

State: ‚ÄúZ‚Äù in ps output.
Fix: Parent must wait() to release entry from process table.

üß† 16. Producer-Consumer Problem (Semaphore Solution)
sem_t empty, full, mutex;

void producer() {
    while(true) {
        produce_item();
        sem_wait(&empty);
        sem_wait(&mutex);
        insert_item();
        sem_post(&mutex);
        sem_post(&full);
    }
}

‚ö° 17. Difference between Cache and Buffer

Cache: Stores frequently accessed data for faster reads (CPU cache, page cache).

Buffer: Temporary storage for data transfer between devices (disk I/O buffer).

üíª 18. How to minimize latency in a multi-threaded system (like Adobe‚Äôs real-time apps)?

Use lock-free structures (std::atomic, compare_exchange).

Pin threads to cores (NUMA awareness).

Avoid page faults ‚Üí lock memory (mlock()).

Use huge pages ‚Üí fewer TLB misses.

Use real-time scheduling (SCHED_FIFO).

üß± 19. Difference between Monolithic and Microkernel
Feature	Monolithic	Microkernel
Architecture	Everything in kernel (Linux)	Minimal kernel, rest in user space
Performance	Fast	Slower (more context switches)
Reliability	Less (crash affects all)	Better isolation
üß† 20. Explain how a file read works in OS

Application calls read(fd, buf, size).

System call ‚Üí switch to kernel mode.

OS checks file descriptor table ‚Üí finds inode.

File system retrieves data ‚Üí may read from disk (via I/O scheduler).

Data copied to buffer ‚Üí returned to user space.

21 . The C++ compilation process typically involves four main stages:-

1.Preprocessing: This is the first stage where the preprocessor handles directives in the source code.
Macro expansion: Replaces macros (defined with #define) with their corresponding values.
File inclusion: Includes the content of header files (specified with #include) into the source file.
Conditional compilation: Processes code blocks based on conditions (e.g., #ifdef, #ifndef).
The output of this stage is an expanded source file, typically with a .i extension.

2.Compilation: In this stage, the compiler translates the preprocessed C++ code into assembly language.
Lexical analysis (tokenization): Breaks down the source code into a stream of tokens (keywords, identifiers, operators, etc.).
Syntax analysis (parsing): Checks if the token sequence conforms to the C++ grammar and builds an Abstract Syntax Tree (AST).
Semantic analysis: Performs type checking, name resolution, and other semantic checks to ensure the code's meaning is consistent and valid.
Intermediate Code Generation: Generates an intermediate representation of the code, which is platform-independent.
Optimization: Applies various techniques to improve the performance and efficiency of the generated code.
The output of this stage is an assembly language file, typically with a .s extension.

3.Assembly: The assembler takes the assembly language file and converts it into machine code.
It translates assembly instructions into their corresponding binary machine code instructions.
This process also involves creating an object file, which contains the machine code along with other information like symbol tables and relocation information.
The output of this stage is an object file, typically with a .o (Linux/macOS) or .obj (Windows) extension.

4.Linking: This is the final stage where the linker combines one or more object files with necessary library files to create a single executable program.
Symbol resolution: Resolves references to external functions and variables by finding their definitions in other object files or libraries.
Relocation: Adjusts addresses within the object files to ensure all parts of the program are correctly linked together in memory.
The output of this stage is an executable file, which can be run directly by the operating system.

22. The C++ process memory layout : - 
refers to how a program's memory is organized in a virtual address space during execution. This organization is typically divided into several distinct segments:

1.Text Segment (Code Segment):
Stores the compiled machine code of the program, including functions and instructions. 
Usually read-only to prevent accidental modification during runtime.
Its size depends on the complexity and size of the program's code.

2.Data Segment:
Stores global and static variables of the program.
Retains values throughout program execution.
Divided into two parts:
Initialized Data Segment: Contains global and static variables that are explicitly initialized by the programmer. 
Uninitialized Data Segment (BSS Segment): Holds global and static variables that are not explicitly initialized. These are typically set to zero by the system at runtime. 

3.Stack:
Used for static memory allocation and manages function calls, local variables, and function arguments.
Operates as a Last-In, First-Out (LIFO) structure.
Grows downwards in memory (towards lower addresses).
Each function call creates a "stack frame" containing its local variables and return address.

4.Heap:
Used for dynamic memory allocation, allowing programs to request variable memory sizes at runtime. 
Managed by functions like new, delete, malloc, free, etc.
Grows upwards in memory (towards higher addresses), typically starting after the BSS segment.
Memory allocated on the heap persists until explicitly deallocated or the program terminates.

